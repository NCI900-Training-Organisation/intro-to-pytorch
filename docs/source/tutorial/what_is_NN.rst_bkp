What is a Neural Network?
=========================

.. admonition:: Overview
   :class: Overview

    * **Tutorial:** 30 min
    * **Exercises:** 0 min

        **Objectives:**
            #. Learn the different parts of a neural network

Neural networks (NN) are computational models inspired by the human brain, designed to recognize patterns and
make decisions based on data. They consist of interconnected layers of nodes, or "neurons," which process
and transform input information. Through training, neural networks learn to improve their accuracy in tasks like image recognition, language processing, and more.
Neural networks comprise of layers that perform operations on data.

Neuron
******

In the context of a neural network, a neuron is a fundamental unit that processes inputs to produce an 
output. Let's break down its role and functionality step by step:

1. **Inputs and Weights**: Each neuron receives multiple inputs. These inputs are each associated with a weight, which is a numerical value that adjusts the importance of the corresponding input. 

2. **Calculating the Weighted Sum**: For each neuron, you first multiply each input by its corresponding weight. Then, you sum up all these weighted inputs. This sum represents the combined influence of all the inputs on the neuron.

3. **Adding Bias**: To this weighted sum, you add a bias term. The bias is another adjustable parameter that helps the neuron model more complex patterns by shifting the activation function's input.

4. **Activation Function**: Finally, you apply an activation function to the resulting value (the weighted sum plus bias). The activation function introduces non-linearity into the neuron's output, which allows the network to learn and represent more complex patterns and relationships.

Putting It All Together:

- **Inputs**: \( x_1, x_2, \ldots, x_n \)
- **Weights**: \( w_1, w_2, \ldots, w_n \)
- **Bias**: \( b \)

**Step-by-Step Process**:
1. **Multiply Inputs by Weights**: \( x_1 \times w_1, x_2 \times w_2, \ldots, x_n \times w_n \)
2. **Sum the Products**: \( \text{Sum} = (x_1 \times w_1) + (x_2 \times w_2) + \ldots + (x_n \times w_n) \)
3. **Add Bias**: \( \text{Weighted Sum} = \text{Sum} + b \)
4. **Apply Activation Function**: The activation function, such as a sigmoid or ReLU (Rectified Linear Unit), is applied to the weighted sum to get the final output.

.. image:: ../figs/neuron.drawio.png

The activation function determines the range of the neuron's output. For example:
- A **sigmoid** activation function outputs values between 0 and 1.
- A **ReLU** activation function outputs values between 0 and positive infinity (with negative inputs mapped to 0).

In summary, a neuron in a neural network processes its inputs through a series of steps—weighted 
multiplication, summation, bias addition, and activation function application—to produce an output value. 
This output is then used in further computations within the network or as the final prediction, depending 
on the network's structure.

Activation Function
********************

Activation functions are crucial components of neural networks, performing several key roles that 
influence the network's ability to learn and make predictions. 

Role of Activation Functions:

1. **Produce Outputs of Neurons**:
   After computing the weighted sum of inputs and adding the bias, an activation function is applied to 
   this value. .

2. **Update Weights and Biases During Training**:
   Activation functions play a role in updating weights and biases during the training process. When the 
   network is trained using methods like backpropagation, the gradient of the loss function with respect 
   to the weights and biases is calculated. The gradient depends on the derivative of the activation 
   function, which helps adjust weights and biases to minimize the error. Therefore, the choice of 
   activation function affects how effectively the network learns.

Characteristics of Activation Functions:

1. **Scale and Normalize Outputs**:
   Activation functions often scale and normalize the neuron's output. 
2. **Introduce Non-Linearity**:
   Activation functions introduce non-linearity into the network. Without non-linearity, even a 
   multi-layer network would behave like a single-layer network, as linear combinations of linear 
   functions are still linear. Non-linearity allows the network to learn and model complex patterns 
   and relationships in the data.

3. **Define Range of Outputs**:
   For instance, The sigmoid activation function outputs values between 0 and 1. It is defined 
   as \( \sigma(x) = \frac{1}{1 + e^{-x}} \). 
   
4. **Simple Derivatives**:
   Most activation functions have simple derivatives, which makes them computationally efficient during the training process. For instance:
   - The derivative of the **sigmoid** function \( \sigma(x) \) is \( \sigma(x) \times (1 - \sigma(x)) \).
   - The derivative of the **ReLU** (Rectified Linear Unit) function, which is \( \max(0, x) \), is 1 for positive inputs and 0 for negative inputs.

Network of Neurons
*******************

A neural network is a complex system of interconnected neurons organized into layers. Each layer's output 
serves as the input for the next layer, creating a stack of neurons that processes data in stages. 
Essentially, a neural network is a sophisticated function that maps inputs to outputs through numerous 
parameters. Training involves adjusting these parameters to improve the network's performance and accuracy.

.. image:: ../figs/layers.png

A neural network consists of three types of layers: input, hidden, and output. The input layer receives 
and holds raw data, with each neuron representing a feature of the data. Hidden layers process this data 
by applying weights, biases, and activation functions to extract and learn complex patterns. These layers 
transform the data and pass it to the next layer in the network. The output layer produces the final 
prediction or classification result based on the processed information from the hidden layers. Each layer 
type plays a crucial role in enabling the network to learn from and make accurate predictions on the data.

.. admonition:: Explanation
   :class: attention

   Matrix X  represents the input matrix, where each column vector corresponds to an input data 
   item. Matrix W contains the weights applied, with each column vector indicating the weights 
   assigned to each feature in the input. Matrix B contains the biases applied to each feature.

   .. math::
      
      X = \begin{bmatrix}
              x_{1}^{(1)} & x_{1}^{(2)}  & x_{1}^{(3)} & .... & x_{1}^{(m)} \\
              x_{2}^{(1)} & x_{2}^{(2)}  & x_{3}^{(3)} & .... & x_{2}^{(m)} \\
              x_{3}^{(1)} & x_{3}^{(2)}  & x_{3}^{(3)} & .... & x_{3}^{(m)} \\ 
              .           & .            & .           & .... & .           \\
              .           & .            & .           & .... & .           \\
              x_{n}^{(1)} & x_{n}^{(2)}  & x_{n}^{(3)} & .... & x_{n}^{(m)} 
          \end{bmatrix}

   :math:`X^{(1)}` will represent the entire vector n x 1 vector representing first data sample while 
   :math:`x_{3}^{(1)}` will represent the third feature in first data sample.

   The figure below illustrates a 2-layer neural network where a single data sample is provided as input.
   The input layer is not counted as one of the layers.

   .. image:: ../figs/2layer_NN.drawio.png

   Each hidden layer produces activations: in this example, layer 1 has 4 activations, while 
   layer 2 has only one activation.

   .. math::
      a^{[1]} = \begin{bmatrix}
              a_{1}^{[1]} \\
              a_{2}^{[1]} \\
              a_{3}^{[1]} \\ 
              a_{4}^{[1]}  
          \end{bmatrix}

   .. math::
      a^{[2]} = a_{1}^{[2]}
      
   
   Each neuron in every layer computes the Z value for each input sample and then calculates the 
   activation. The figure illustrates this process with an example of a first neuron in layer 1 
   processing the first input sample.

   .. image:: ../figs/activation.drawio.png

   In this situation the we are calculating
   
   .. math::
      
      a_{1}^{[1]} = f(Z_{1}^{[1]}) = W_{1}^{[1]T} + b_{1}^{[1]}

   .. math::

      a_{2}^{[1]} = f(Z_{2}^{[1]}) = W_{2}^{[1]T} + b_{2}^{[1]}

   .. math::

      a_{3}^{[1]} = f(Z_{3}^{[1]}) = W_{3}^{[1]T} + b_{3}^{[1]}

   Where :math:`W_{1}^{[1]T}, W_{2}^{[1]T}, W_{3}^{[1]T}` are transpose of vectors of size :math:`(3 \times 1)`.

   In :math:`Z_{1}^{[1]}` the superscript value in square brackets represents the 
   layer number, and the subscript indicates the neuron within that layer.

   We can stack the weights in each neuron to a matrix.

   

   .. math::

      W = \begin{bmatrix}
            ------ W_{1}^{[1]T} ------- \\  
            ------ W_{2}^{[1]T} ------- \\ 
            ------ W_{3}^{[1]T} ------- 
          \end{bmatrix}
          
   Similarly we can stack the bias

   .. math::

      B = \begin{bmatrix}
               b_{1}^{[1]} \\
               b_{2}^{[1]} \\ 
               b_{3}^{[1]} 
          \end{bmatrix}

   and the operation 

   .. math::

      Z = W^{T} \times X + B 

   corresponds to the calculations

   .. math::
      
      Z^{[1]} = \begin{bmatrix}
                  Z_{1}^{[1]} \\  
                  Z_{2}^{[1]} \\ 
                  Z_{3}^{[1]} 
               \end{bmatrix}

   .. math::

            =  \begin{bmatrix}
                  W_{1}^{[1]T} \times X^{(1)} + b_{1}^{[1]} \\  
                  W_{2}^{[1]T} \times X^{(1)} + b_{2}^{[1]} \\ 
                  W_{3}^{[1]T} \times X^{(1)} + b_{3}^{[1]} 
               \end{bmatrix}

        

   and finally we apply the activation function to the above matrix

   .. math::

      A^{[1]} = \begin{bmatrix}
                  A_{1}^{[1]} \\  
                  A_{2}^{[1]} \\ 
                  A_{3}^{[2]}  
               \end{bmatrix}
   .. math::

            = \begin{bmatrix}
                  f(Z_{1}^{[1]}) \\  
                  f(Z_{2}^{[1]}) \\ 
                  f(Z_{3}^{[1]}) 
               \end{bmatrix}

   where f() could be a ReLU or a Sigmoid function.

   The above example shows the application of single input sample on the neuwral netwrk. If 
   we have N samples using vector operations we can finally compute the the complete Matrix A as

   .. math::
      
      A^{[1]} = \begin{bmatrix}
                  a_{1}^{[1]} & a_{1}^{[2]} & .... & a_{1}^{[m]} \\  
                  a_{2}^{[1]} & a_{2}^{[2]} & .... & a_{2}^{[m]} \\ 
                  a_{3}^{[1]} & a_{3}^{[2]} & .... & a_{3}^{[m]} 
               \end{bmatrix}

   This matrix then form the input to the next layer.

   .. math::

      Z = \begin{bmatrix}
              \| & \|  \\
              z^{1} & z^{2}  \\
              \| & \|  
          \end{bmatrix}

   Matrix A contains the values after the activation function has been applied.

   .. math::

      A = f(Z)

   .. math::
      A = \begin{bmatrix}
              \| & \|  \\
              a^{1} & a^{2}  \\
              \| & \|  
          \end{bmatrix}


   For example, consider a hidden layer with 4 neurons. Also suppose we just have two data items,
   each with 4 features. The calculations for that layer will proceed as follows:

   .. math::

      X = \begin{bmatrix}
              x_{1}^{1} & x_{1}^{2}  \\
              x_{2}^{1} & x_{2}^{2}  \\
              x_{3}^{1} & x_{3}^{2}  \\
              x_{4}^{1} & x_{4}^{2}  
          \end{bmatrix}

   .. math::

      W = \begin{bmatrix}
              w_{1}^{1} & w_{1}^{2}  \\
              w_{2}^{1} & w_{2}^{2}  \\
              w_{3}^{1} & w_{3}^{2}  \\
              w_{4}^{1} & w_{4}^{2}
          \end{bmatrix}

   .. math::

      W^{T} * X =  

   .. math::

         \begin{bmatrix}
            w_{1}^{1}.x_{1}^{1} +  w_{2}^{1}.x_{2}^{1} + w_{3}^{1}.x_{3}^{1} + w_{4}^{1}.x_{4}^{1}  &  w_{1}^{1}.x_{1}^{2} +  w_{2}^{1}.x_{2}^{2} + w_{3}^{1}.x_{3}^{2} + w_{4}^{1}.x_{4}^{2} \\
            w_{1}^{2}.x_{1}^{1} +  w_{2}^{2}.x_{2}^{1} + w_{3}^{2}.x_{3}^{1} + w_{4}^{2}.x_{4}^{1}  &  w_{1}^{2}.x_{1}^{2} +  w_{2}^{2}.x_{2}^{2} + w_{3}^{2}.x_{3}^{2} + w_{4}^{2}.x_{4}^{2} \\
         \end{bmatrix}

   .. math::

      Z = W^{T} * X + B =  

   .. math::

      \begin{bmatrix}
         w_{1}^{1}.x_{1}^{1} +  w_{2}^{1}.x_{2}^{1} + w_{3}^{1}.x_{3}^{1} + w_{4}^{1}.x_{4}^{1} + b_{1} &  w_{1}^{1}.x_{1}^{2} +  w_{2}^{1}.x_{2}^{2} + w_{3}^{1}.x_{3}^{2} + w_{4}^{1}.x_{4}^{2} + b_{1} \\
         w_{1}^{2}.x_{1}^{1} +  w_{2}^{2}.x_{2}^{1} + w_{3}^{2}.x_{3}^{1} + w_{4}^{2}.x_{4}^{1} + b_{2} &  w_{1}^{2}.x_{1}^{2} +  w_{2}^{2}.x_{2}^{2} + w_{3}^{2}.x_{3}^{2} + w_{4}^{2}.x_{4}^{2} + b_{2} \\
      \end{bmatrix}
   

   .. math::

      A = f( W^{T} * X + B ) = 

   where f() can be any activation function, such as ReLU or Sigmoid function.

   .. math::

         \begin{bmatrix}
            f(w_{1}^{1}.x_{1}^{1} +  w_{2}^{1}.x_{2}^{1} + w_{3}^{1}.x_{3}^{1} + w_{4}^{1}.x_{4}^{1} + b_{1}) &  f(w_{1}^{1}.x_{1}^{2} +  w_{2}^{1}.x_{2}^{2} + w_{3}^{1}.x_{3}^{2} + w_{4}^{1}.x_{4}^{2} + b_{1}) \\
            f(w_{1}^{2}.x_{1}^{1} +  w_{2}^{2}.x_{2}^{1} + w_{3}^{2}.x_{3}^{1} + w_{4}^{2}.x_{4}^{1} + b_{2}) &  f(w_{1}^{2}.x_{1}^{2} +  w_{2}^{2}.x_{2}^{2} + w_{3}^{2}.x_{3}^{2} + w_{4}^{2}.x_{4}^{2} + b_{2}) \\
         \end{bmatrix}


   *In matrix A, the horizontal axis represents the training samples, while the vertical axis 
   represents the neurons in a layer.*

               

Cost Functions
**************

We typically initialize random weights for each neuron. Then, using the above function, calculations 
are propagated from one layer to the next, a process known as the forward pass. The cost function 
measures the network's performance by evaluating how well it predicts the output after this forward pass. 
It penalizes large errors and adjusts for smaller errors to improve the model's accuracy.

The network performs the following steps:

1. Inputs the data.
2. Executes a forward pass to generate the network's output.
3. Computes the error in the output using the loss function. 

An epoch refers to a full pass through the entire training dataset, including performing forward passes 
for all data samples.

.. image:: ../figs/loss.png

We then compute the difference between the expected output and the output received from the network.
As expected, the output using random weights will be poor.


Since errors can be both positive and negative, we want to ensure they don't cancel each other out. 
Therefore, we typically use the square of the error or the absolute value to avoid this issue.


.. admonition:: Explanation
   :class: attention

      Mean Squared Error (MSE) is a common cost function.
      .. math::

         MSE = \frac{1}{2} * \sum_{n=1}^{m} (y_{train} - y_{network})^{2} 


Backpropagation
***************

Based on the loss function we may to excite (incraese the influence ) or inhibit (decrtrease the influene)
of some neurons. To do this each layers indirectly influences the weights and biases of the prior layer
in the network. This is called backpropagation. 





.. admonition:: Key Points
   :class: hint

      #. At its core, a neural network performs general matrix-matrix operations (GEMM).
      #. After each epoch, weights are adjusted to recalibrate the network.
      #. The more data you have, the more effective this recalibration becomes (brute force approach).