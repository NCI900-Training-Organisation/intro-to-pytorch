#!/bin/bash

#PBS -P vp91
#PBS -q gpuvolta

#PBS -l ncpus=96
#PBS -l ngpus=8
#PBS -l mem=10GB
#PBS -l walltime=00:05:00 

#PBS -N multinode

module load python3/3.11.0  
module load cuda/12.3.2
module load nccl/2.19.4

. /scratch/vp91/Training-Venv/pytorch/bin/activate

python3 /scratch/vp91/$USER/intro-to-pytorch/src/distributed_data_parallel.py

# Get the list of allocated nodes
NODES=$(cat $PBS_NODEFILE | uniq)
NODE_ARR=($NODES)



# Define the master node (usually the first node in the list)
MASTER_ADDR=${NODE_ARR[0]}
MASTER_PORT=12355  # Set an appropriate port for communication

NNODES=2
NPROC_PER_NODE=4
WORLD_SIZE=$(($NNODES * $NPROC_PER_NODE))

# Rendezvous backend and endpoint
RDZV_BACKEND="c10d"
RDZV_ENDPOINT="${MASTER_ADDR}:${MASTER_PORT}"
RDZV_ID="100"

torchrun --nnodes=$NNODES --nproc_per_node=$NPROC_PER_NODE \
         --rdzv_backend=$RDZV_BACKEND --rdzv_endpoint=$RDZV_ENDPOINT --rdzv_id=$RDZV_ID \
         /scratch/vp91/$USER/intro-to-pytorch/src/multinode_torchrun.py