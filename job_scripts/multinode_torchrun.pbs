#!/bin/bash

#PBS -P vp91
#PBS -q gpuvolta

#PBS -l ncpus=96
#PBS -l ngpus=8
#PBS -l mem=10GB
#PBS -l walltime=00:05:00 

#PBS -N multinode

module load python3/3.11.0  
module load cuda/12.3.2
module load nccl/2.19.4

. /scratch/vp91/Training-Venv/pytorch/bin/activate

python3 /scratch/vp91/$USER/intro-to-pytorch/src/distributed_data_parallel.py

# Get the list of allocated nodes
NODES=$(cat $PBS_NODEFILE | uniq)
NODE_ARR=($NODES)

# Define the master node (usually the first node in the list)
MASTER_ADDR=${NODE_ARR[0]}
MASTER_PORT=12355  # Set an appropriate port for communication

NNODES=2
NPROC_PER_NODE=4
WORLD_SIZE=$(($NNODES * $NPROC_PER_NODE))

torchrun --nnodes=$NNODES --nproc_per_node=$NPROC_PER_NODE \
         --node_rank=$PBS_NODEID --master_addr=$MASTER_ADDR \
         --master_port=$MASTER_PORT /scratch/vp91/$USER/intro-to-pytorch/src/multinode_torchrun.py