{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cdb801e-e281-476f-b3e9-e470785d3ad9",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96127ef4-bb03-492f-81b9-672f74c20b5c",
   "metadata": {},
   "source": [
    "Neural networks are computational models inspired by the human brain, designed to recognize patterns and\n",
    "make decisions based on data. They consist of interconnected layers of nodes, or \"neurons,\" which process\n",
    "and transform input information. Through training, neural networks learn to improve their accuracy in tasks like image recognition, language processing, and more.Neural networks comprise of layers that perform operations on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a40db4-da7b-4d24-b707-a39b79d2440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# The jupyter notebook is launched from your $HOME directory.\n",
    "# Change the working directory to the workshop directory\n",
    "# which was created in your username directory under /scratch/vp91\n",
    "os.chdir(os.path.expandvars(\"/scratch/vp91/$USER/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604d5312-0b33-4162-b5f4-551c21732550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1baab1-a7b6-429e-afa3-822e61da46ad",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The Pima Indians Diabetes dataset is a popular dataset in the field of machine learning and statistics, particularly for those working on classification problems. \n",
    "\n",
    "Dataset Overview:\n",
    "**Source**: The dataset was created by the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK) and is available in the UCI Machine Learning Repository.\n",
    "**Purpose**: The dataset is used to predict the onset of diabetes within five years based on diagnostic measures.\n",
    "**Features**: The dataset contains 768 samples, each with 8 features. \n",
    "\n",
    "The features are:\n",
    "\n",
    "1. Pregnancies: Number of times pregnant.\n",
    "2. Glucose: Plasma glucose concentration (mg/dL) a 2 hours in an oral glucose tolerance test.\n",
    "3. Blood Pressure: Diastolic blood pressure (mm Hg) at the time of screening.\n",
    "4. Skin Thickness: Triceps skinfold thickness (mm) measured at the back of the upper arm.\n",
    "5. Insulin: 2-Hour serum insulin (mu U/ml).\n",
    "6. BMI: Body mass index (weight in kg/(height in m)^2).\n",
    "7. Diabetes Pedigree Function: A function that scores likelihood of diabetes based on family history.\n",
    "8. Age: Age of the individual (years).\n",
    "\n",
    "**Outcome**: Whether or not the individual has diabetes (1 for positive, 0 for negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d4b1b9b-bf50-4867-8345-43a7106a25da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,148,72,35,0,33.6,0.627,50,1\n",
      "1,85,66,29,0,26.6,0.351,31,0\n",
      "8,183,64,0,0,23.3,0.672,32,1\n",
      "1,89,66,23,94,28.1,0.167,21,0\n",
      "0,137,40,35,168,43.1,2.288,33,1\n",
      "5,116,74,0,0,25.6,0.201,30,0\n",
      "3,78,50,32,88,31.0,0.248,26,1\n",
      "10,115,0,0,0,35.3,0.134,29,0\n",
      "2,197,70,45,543,30.5,0.158,53,1\n",
      "8,125,96,0,0,0.0,0.232,54,1\n"
     ]
    }
   ],
   "source": [
    "!head /scratch/vp91/$USER/intro-to-pytorch/data/pima-indians-diabetes.data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee303492-97bf-4274-9a14-04c1c116f6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/vp91/jxj900/intro-to-pytorch/data/pima-indians-diabetes.data.csv\n"
     ]
    }
   ],
   "source": [
    "datapath = os.path.expandvars('/scratch/vp91/$USER/intro-to-pytorch/data/pima-indians-diabetes.data.csv')\n",
    "print(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c13d2-bee6-436c-b838-2c8e04a24ec6",
   "metadata": {},
   "source": [
    "### Curate the dataset\n",
    "Load the dataset, split into features (X) and output (y) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "222a7a99-2723-486d-9a1e-58d2792c84e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(datapath, delimiter=',')\n",
    "X = dataset[:,0:8] \n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a20b9-0c73-4995-b772-0e773cc03c8b",
   "metadata": {},
   "source": [
    "### Convert the data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c45e8f-894e-46d3-84c1-25fbca333f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc70a8b-2c37-4c09-bd7c-717d556cb39c",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "\n",
    "When designing the model, keep the following points in mind:\n",
    "\n",
    "1. The input features in the input layer must match the input features in the dataset (`X_tensor`).\n",
    "2. A high number of layers can increase computation time, while too few layers may result in poor predictions.\n",
    "3. Each layer should be followed by an activation function.\n",
    "\n",
    "In this example, we will use a 3-layer neural network:\n",
    "\n",
    "1. The input layer expects 8 features.\n",
    "2. The first hidden layer has 12 neurons, followed by a ReLU activation function.\n",
    "3. The second hidden layer has 8 neurons, followed by another ReLU activation function.\n",
    "4. The output layer has one neuron, followed by a sigmoid activation function.\n",
    "\n",
    "The sigmoid function outputs values between 0 and 1, which is exactly what we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d8051c-32ee-45c1-b797-58c0e68bbcfb",
   "metadata": {},
   "source": [
    "\n",
    "In PyTorch, neural networks can be defined using different approaches, and two common ones are the Sequential model and the class-based model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a5b5b-d3bc-434f-9b92-a8571d7599f5",
   "metadata": {},
   "source": [
    "#### Sequential model\n",
    "\n",
    "* The Sequential model is a simple, linear stack of layers where each layer has a single input and output. It is useful for straightforward feedforward networks where layers are applied in a sequential order.\n",
    "* It is easier to use for simple architectures where layers are applied in a linear fashion.\n",
    "* Defined Using: *torch.nn.Sequential*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cb10442-640a-4ded-a81f-6c2607a86bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = nn.Sequential(\n",
    "\n",
    "    # First hidden layer\n",
    "    # This layer that takes an input of size 8 and outputs 12 neurons. \n",
    "    # The input is a vector of size 8, and the output is a vector \n",
    "    # of size 12. The layer applies a linear transformation to the input data.\n",
    "    nn.Linear(8, 12),\n",
    "\n",
    "    #ReLU (Rectified Linear Unit) activation function\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # Second hidden layer\n",
    "    #This layer takes the 12 neurons from the previous layer as input and outputs a \n",
    "    # vector of size 8\n",
    "    nn.Linear(12, 8),\n",
    "\n",
    "    #ReLU (Rectified Linear Unit) activation function\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # Output hidden layer\n",
    "    # Takes the 8 neurons as input and outputs a single value \n",
    "    # (since the output dimension is 1).\n",
    "    nn.Linear(8, 1),\n",
    "\n",
    "    #Sigmoid activation function\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8647eb7-799b-42e5-b42d-be699b5e5a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8225d-4528-4a2f-a23e-9285d4ab5c8e",
   "metadata": {},
   "source": [
    "### Class-Based Model\n",
    "\n",
    "The class-based model allows you to define a network by subclassing torch.nn.Module. This approach provides greater flexibility and control, making it suitable for complex models and custom behaviors.\n",
    "\n",
    "* Offers full control over the network architecture, including complex data flows, multiple inputs/outputs, and custom forward methods.\n",
    "* Custom Forward Pass: You can define complex forward passes and control data flow through the network.\n",
    "* Dynamic Behavior: Allows for dynamic computations, such as conditional layers or operations.\n",
    "* Defined Using: Subclass of torch.nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "263d5838-320d-4dad-ac59-e2d95ada7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(8, 12)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(12, 8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        \n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "289c55b3-f54b-4a79-ba58-5788237aabb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PimaClassifier(\n",
      "  (hidden1): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (hidden2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (act_output): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class_model = PimaClassifier()\n",
    "print(class_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e889fd5-49a7-4509-a2fc-ef7d5f8c722f",
   "metadata": {},
   "source": [
    "### Define the loss function\n",
    "Binary Cross-Entropy (BCE) Loss: Measures the performance of a classification model whose output is a probability value between 0 and 1. It calculates the difference between the predicted probabilities and the actual binary labels (0 or 1) and penalizes the model more when the predictions are further from the true labels.\n",
    "\n",
    "BCELoss(y', y)=−[ylog(y')+(1−y)log(1−y')]\n",
    "\n",
    "Where, y' is the predicted output and y is the actual otput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "593672e5-4e14-473d-80f9-2ed00c127729",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa2eab-7897-439e-9c20-08eb523ec7d6",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Optimizer's main role is to update the model's parameters based on the gradients computed during backpropagation.\n",
    "\n",
    "1. **Parameter Updates**: Optimizers adjust the weights and biases of the neural network to reduce the loss. This involves applying algorithms that modify the parameters to minimize the difference between the predicted outputs and the actual targets.\n",
    "2. **Learning Rate Management**: Most optimizers include mechanisms to adjust the learning rate, either statically or dynamically, to control how large the parameter updates are.\n",
    "\n",
    "In this example we use an optimizer called Adaptive Moment Estimation (Adam). This computes an adaptive learning rates for each parameter by considering both the mean and the variance of the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "518bf03c-0594-494e-bdb3-a41421ac53a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(class_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608326d-27a3-4a5b-b485-e9af74b0f2e8",
   "metadata": {},
   "source": [
    "#### Training the Model\n",
    "\n",
    "Training a neural network involves epochs and batches, which define how data is fed to the model:\n",
    "\n",
    "- **Epoch:** A full pass through the entire training dataset.\n",
    "- **Batch:** A subset of samples processed at a time, with gradient descent performed after each batch.\n",
    "\n",
    "In practice, the dataset is divided into batches, and each batch is processed sequentially in a training loop. Completing all batches constitutes one epoch. The process is repeated for multiple epochs to refine the model.\n",
    "\n",
    "Batch size is constrained by system memory (GPU memory), and computational demands scale with batch size. More epochs and batches lead to better model performance but increase training time. The optimal number of epochs and batch size is often determined through experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea1f2ef-0b63-435c-a93f-329aa9ae6228",
   "metadata": {},
   "source": [
    "#### Purpose of optimizer.zero_grad(), loss.backward(), optimizer.step()\n",
    "\n",
    "**optimizer.zero_grad()**: During training, gradients accumulate by default in PyTorch. This means that if you don’t clear them, gradients from multiple backward passes (from different batches) will be added together, which can lead to incorrect updates to the model parameters.\n",
    "By calling optimizer.zero_grad(), you ensure that gradients from previous steps are reset to zero, preventing them from affecting the current update.\n",
    "\n",
    "**loss.backward()**:  Calculates the gradients of the loss with respect to each parameter of the model. This is done using backpropagation, a key algorithm for training neural networks.\n",
    "\n",
    "**optimizer.step()**: Used to update the model's parameters based on the gradients computed during during the backward pass (**loss.backward()**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad6b09b5-9e9b-4376-ad89-d1ff8b4791eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest loss 1.0133854150772095\n",
      "Finished epoch 1, latest loss 0.7441348433494568\n",
      "Finished epoch 2, latest loss 0.6407996416091919\n",
      "Finished epoch 3, latest loss 0.5694960951805115\n",
      "Finished epoch 4, latest loss 0.5475271940231323\n",
      "Finished epoch 5, latest loss 0.5354700088500977\n",
      "Finished epoch 6, latest loss 0.523318350315094\n",
      "Finished epoch 7, latest loss 0.5216410756111145\n",
      "Finished epoch 8, latest loss 0.5160097479820251\n",
      "Finished epoch 9, latest loss 0.5094536542892456\n",
      "Finished epoch 10, latest loss 0.5070609450340271\n",
      "Finished epoch 11, latest loss 0.5028208494186401\n",
      "Finished epoch 12, latest loss 0.49739667773246765\n",
      "Finished epoch 13, latest loss 0.4959896504878998\n",
      "Finished epoch 14, latest loss 0.48979130387306213\n",
      "Finished epoch 15, latest loss 0.48862120509147644\n",
      "Finished epoch 16, latest loss 0.48711657524108887\n",
      "Finished epoch 17, latest loss 0.48717817664146423\n",
      "Finished epoch 18, latest loss 0.48560628294944763\n",
      "Finished epoch 19, latest loss 0.48733898997306824\n",
      "Finished epoch 20, latest loss 0.48479729890823364\n",
      "Finished epoch 21, latest loss 0.4859431982040405\n",
      "Finished epoch 22, latest loss 0.48687854409217834\n",
      "Finished epoch 23, latest loss 0.4858115613460541\n",
      "Finished epoch 24, latest loss 0.4867473840713501\n",
      "Finished epoch 25, latest loss 0.48618680238723755\n",
      "Finished epoch 26, latest loss 0.489460289478302\n",
      "Finished epoch 27, latest loss 0.4896836578845978\n",
      "Finished epoch 28, latest loss 0.49068114161491394\n",
      "Finished epoch 29, latest loss 0.4915824234485626\n",
      "Finished epoch 30, latest loss 0.4922678768634796\n",
      "Finished epoch 31, latest loss 0.49436816573143005\n",
      "Finished epoch 32, latest loss 0.4938143491744995\n",
      "Finished epoch 33, latest loss 0.4956669807434082\n",
      "Finished epoch 34, latest loss 0.4964727759361267\n",
      "Finished epoch 35, latest loss 0.49688011407852173\n",
      "Finished epoch 36, latest loss 0.49841198325157166\n",
      "Finished epoch 37, latest loss 0.4984031617641449\n",
      "Finished epoch 38, latest loss 0.4976874589920044\n",
      "Finished epoch 39, latest loss 0.4998432397842407\n",
      "Finished epoch 40, latest loss 0.5000931024551392\n",
      "Finished epoch 41, latest loss 0.49881359934806824\n",
      "Finished epoch 42, latest loss 0.5013794302940369\n",
      "Finished epoch 43, latest loss 0.5018144845962524\n",
      "Finished epoch 44, latest loss 0.5031425356864929\n",
      "Finished epoch 45, latest loss 0.5011367797851562\n",
      "Finished epoch 46, latest loss 0.5017842054367065\n",
      "Finished epoch 47, latest loss 0.5029036402702332\n",
      "Finished epoch 48, latest loss 0.5025818347930908\n",
      "Finished epoch 49, latest loss 0.503027617931366\n",
      "Finished epoch 50, latest loss 0.5025387406349182\n",
      "Finished epoch 51, latest loss 0.5040374994277954\n",
      "Finished epoch 52, latest loss 0.5043436288833618\n",
      "Finished epoch 53, latest loss 0.5025737881660461\n",
      "Finished epoch 54, latest loss 0.5033250451087952\n",
      "Finished epoch 55, latest loss 0.5042853951454163\n",
      "Finished epoch 56, latest loss 0.5038459897041321\n",
      "Finished epoch 57, latest loss 0.5040754675865173\n",
      "Finished epoch 58, latest loss 0.5030184388160706\n",
      "Finished epoch 59, latest loss 0.5044047832489014\n",
      "Finished epoch 60, latest loss 0.504377007484436\n",
      "Finished epoch 61, latest loss 0.5010861158370972\n",
      "Finished epoch 62, latest loss 0.5017938017845154\n",
      "Finished epoch 63, latest loss 0.5012050271034241\n",
      "Finished epoch 64, latest loss 0.5012074708938599\n",
      "Finished epoch 65, latest loss 0.49953681230545044\n",
      "Finished epoch 66, latest loss 0.5010402202606201\n",
      "Finished epoch 67, latest loss 0.4998503625392914\n",
      "Finished epoch 68, latest loss 0.49927106499671936\n",
      "Finished epoch 69, latest loss 0.4989463984966278\n",
      "Finished epoch 70, latest loss 0.49754664301872253\n",
      "Finished epoch 71, latest loss 0.49865809082984924\n",
      "Finished epoch 72, latest loss 0.4983295500278473\n",
      "Finished epoch 73, latest loss 0.4982421100139618\n",
      "Finished epoch 74, latest loss 0.49736273288726807\n",
      "Finished epoch 75, latest loss 0.49783962965011597\n",
      "Finished epoch 76, latest loss 0.49796411395072937\n",
      "Finished epoch 77, latest loss 0.49635088443756104\n",
      "Finished epoch 78, latest loss 0.49592819809913635\n",
      "Finished epoch 79, latest loss 0.4956660270690918\n",
      "Finished epoch 80, latest loss 0.49601975083351135\n",
      "Finished epoch 81, latest loss 0.49516162276268005\n",
      "Finished epoch 82, latest loss 0.4952187240123749\n",
      "Finished epoch 83, latest loss 0.49543410539627075\n",
      "Finished epoch 84, latest loss 0.4951004981994629\n",
      "Finished epoch 85, latest loss 0.4959757328033447\n",
      "Finished epoch 86, latest loss 0.49590548872947693\n",
      "Finished epoch 87, latest loss 0.4979358911514282\n",
      "Finished epoch 88, latest loss 0.4976203739643097\n",
      "Finished epoch 89, latest loss 0.496722936630249\n",
      "Finished epoch 90, latest loss 0.49824845790863037\n",
      "Finished epoch 91, latest loss 0.49991005659103394\n",
      "Finished epoch 92, latest loss 0.49737784266471863\n",
      "Finished epoch 93, latest loss 0.5004721879959106\n",
      "Finished epoch 94, latest loss 0.49876901507377625\n",
      "Finished epoch 95, latest loss 0.5004343390464783\n",
      "Finished epoch 96, latest loss 0.5003745555877686\n",
      "Finished epoch 97, latest loss 0.5005397200584412\n",
      "Finished epoch 98, latest loss 0.5013123750686646\n",
      "Finished epoch 99, latest loss 0.5022812485694885\n",
      "CPU times: user 6.78 s, sys: 131 ms, total: 6.91 s\n",
      "Wall time: 7.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_epochs = 100\n",
    "batch_size = 10\n",
    " \n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X_tensor), batch_size):\n",
    "\n",
    "        # For each mini-batch, the input data (Xbatch) is transferred to the GPU (device), \n",
    "        # allowing the computations to run on the GPU for faster training.\n",
    "        Xbatch = X_tensor[i:i+batch_size]\n",
    "\n",
    "        # The model takes the input mini-batch (Xbatch) and makes predictions (y_pred).\n",
    "        y_pred = class_model(Xbatch)\n",
    "\n",
    "        # The corresponding expected output labels for the current mini-batch are also \n",
    "        # transferred to the GPU.\n",
    "        ybatch = y_tensor[i:i+batch_size]\n",
    "\n",
    "        # The loss function calculates the error (loss) between the predicted values (y_pred) \n",
    "        # and the actual labels (ybatch).\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "\n",
    "        # Before backpropagation, the gradients of the model parameters are reset to zero. \n",
    "        # This ensures that the gradients from the previous iteration do not accumulate.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backpropagation is performed to compute the gradients of the loss with respect to \n",
    "        # the model parameters.\n",
    "        loss.backward()\n",
    "\n",
    "        # The optimizer updates the model's parameters using the computed gradients.\n",
    "        optimizer.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b239a9-7e1c-42ba-8b54-58c79d26986b",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "\n",
    "Currently, we are testing the model on the training dataset. Ideally, we should split the data into separate training and testing datasets, or use a distinct dataset for evaluation. For simplicity, we are testing the model on the same data used for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89e28fe2-90c5-4cd4-bd37-30ebe9183772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7447916865348816\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = class_model(X_tensor)\n",
    " \n",
    "accuracy = (y_pred.round() == y_tensor).float().mean()\n",
    "print(f\"Accuracy {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a44bb-25c6-4f67-8a34-514d7eadbbaf",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. **Increase the number of layers in the neural network.** Observe any changes in accuracy.\n",
    "2. **Change the optimizer from Adam to [Stochastic Gradient Descent (SGD)](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html).** Evaluate how this affects the loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84664807-8163-46dd-b037-1b3f73d8cbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
