{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cdb801e-e281-476f-b3e9-e470785d3ad9",
   "metadata": {},
   "source": [
    "### Using Multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d5312-0b33-4162-b5f4-551c21732550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a40db4-da7b-4d24-b707-a39b79d2440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# The jupyter notebook is launched from your $HOME directory.\n",
    "# Change the working directory to the workshop directory\n",
    "# which was created in your username directory under /scratch/vp91\n",
    "os.chdir(os.path.expandvars(\"/scratch/vp91/$USER/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098c4ec-368b-4802-9800-fc4c4b7479ba",
   "metadata": {},
   "source": [
    "#### Set Device\n",
    "Se the default device as the GPU if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51be64-542f-401c-ae73-00da2bbd6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gpus = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "datapath = os.path.expandvars('/scratch/vp91/$USER/intro-to-pytorch/data/pima-indians-diabetes.data.csv')\n",
    "\n",
    "# Define the custom Dataset class\n",
    "column_names = [\n",
    "    'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
    "    'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f944a147-3f5e-4c42-b142-850d04458270",
   "metadata": {},
   "source": [
    "### Process Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50696138-0c0d-4a0b-aa80-ed73cff87fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    \n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d29976e-56b5-46ce-8743-5480524bbca1",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28a5d7-0d69-47c9-ba24-995f02168856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom Dataset class\n",
    "class PimaDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        # Load the CSV file without header and assign column names\n",
    "        self.data = pd.read_csv(csv_file, header=None, names=column_names)\n",
    "        self.features = self.data.drop('Outcome', axis=1).values\n",
    "        self.labels = self.data['Outcome'].values\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        self.features_tensor = torch.tensor(self.features, dtype=torch.float32)\n",
    "        self.labels_tensor = torch.tensor(self.labels, dtype=torch.long)\n",
    "        \n",
    "        # Calculate mean and std\n",
    "        self.mean = self.features_tensor.mean(dim=0)\n",
    "        self.std = self.features_tensor.std(dim=0)\n",
    "        \n",
    "        # Normalize the features\n",
    "        self.features_tensor = (self.features_tensor - self.mean) / self.std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features_tensor[idx]\n",
    "        label = self.labels_tensor[idx]\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4ebf96-12bc-4520-bbcb-690e5edebac9",
   "metadata": {},
   "source": [
    "### Split the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca23e3-f8d9-4818-93e4-fa9304792335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(rank, world_size, batch_size=32, pin_memory=False, num_workers=0):\n",
    "    dataset = PimaDataset(datapath)\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, num_workers=num_workers, drop_last=False, shuffle=False, sampler=sampler)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc70a8b-2c37-4c09-bd7c-717d556cb39c",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d5838-320d-4dad-ac59-e2d95ada7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(8, 12)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(12, 8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec5647-aded-4179-89c0-0c5d44b0c6db",
   "metadata": {},
   "source": [
    "#### Wrap model in DDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc9a702-4b3a-423f-80d3-79d1e3d9e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(rank, world_size):\n",
    "\n",
    "    # setup the process groups\n",
    "    setup(rank, world_size)\n",
    "    # prepare the dataloader\n",
    "    dataloader = prepare(rank, world_size)\n",
    "    \n",
    "    # instantiate the model(it's your own model) and move it to the right device\n",
    "    model = PimaClassifier().to(rank)\n",
    "    \n",
    "    # wrap the model with DDP\n",
    "    # device_ids tell DDP where is your model\n",
    "    # output_device tells DDP where to output, in our case, it is rank\n",
    "    # find_unused_parameters=True instructs DDP to find unused output of the forward() function of any module in the model\n",
    "    model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)\n",
    "\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    n_epochs = 100\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # if we are using DistributedSampler, we have to tell it which epoch this is\n",
    "        dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        for batch_features, batch_labels in dataloader:\n",
    "            batch_features = batch_features.to(rank)\n",
    "            batch_labels = batch_labels.to(rank)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            outputs = model(batch_features)\n",
    "        \n",
    "            batch_labels = batch_labels.unsqueeze(1).float()\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70bfdf8-9619-4448-ad45-cb2277d937ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593672e5-4e14-473d-80f9-2ed00c127729",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    world_size = nb_gpus    \n",
    "    mp.spawn(main, args=(world_size,), nprocs=world_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a44bb-25c6-4f67-8a34-514d7eadbbaf",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. **What is the time difference in training**? Compare it with the previous training (change epoch to 100)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
