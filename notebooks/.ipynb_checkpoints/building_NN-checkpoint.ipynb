{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cdb801e-e281-476f-b3e9-e470785d3ad9",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96127ef4-bb03-492f-81b9-672f74c20b5c",
   "metadata": {},
   "source": [
    "Neural networks are computational models inspired by the human brain, designed to recognize patterns and\n",
    "make decisions based on data. They consist of interconnected layers of nodes, or \"neurons,\" which process\n",
    "and transform input information. Through training, neural networks learn to improve their accuracy in tasks like image recognition, language processing, and more.Neural networks comprise of layers that perform operations on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a40db4-da7b-4d24-b707-a39b79d2440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# The jupyter notebook is launched from your $HOME directory.\n",
    "# Change the working directory to the workshop directory\n",
    "# which was created in your username directory under /scratch/vp91\n",
    "os.chdir(os.path.expandvars(\"/scratch/vp91/$USER/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d5312-0b33-4162-b5f4-551c21732550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1baab1-a7b6-429e-afa3-822e61da46ad",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The Pima Indians Diabetes dataset is a popular dataset in the field of machine learning and statistics, particularly for those working on classification problems. \n",
    "\n",
    "Dataset Overview:\n",
    "**Source**: The dataset was created by the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK) and is available in the UCI Machine Learning Repository.\n",
    "**Purpose**: The dataset is used to predict the onset of diabetes within five years based on diagnostic measures.\n",
    "**Features**: The dataset contains 768 samples, each with 8 features. \n",
    "\n",
    "The features are:\n",
    "\n",
    "1. Pregnancies: Number of times pregnant.\n",
    "2. Glucose: Plasma glucose concentration (mg/dL) a 2 hours in an oral glucose tolerance test.\n",
    "3. Blood Pressure: Diastolic blood pressure (mm Hg) at the time of screening.\n",
    "4. Skin Thickness: Triceps skinfold thickness (mm) measured at the back of the upper arm.\n",
    "5. Insulin: 2-Hour serum insulin (mu U/ml).\n",
    "6. BMI: Body mass index (weight in kg/(height in m)^2).\n",
    "7. Diabetes Pedigree Function: A function that scores likelihood of diabetes based on family history.\n",
    "8. Age: Age of the individual (years).\n",
    "\n",
    "**Outcome**: Whether or not the individual has diabetes (1 for positive, 0 for negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b1b9b-bf50-4867-8345-43a7106a25da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /scratch/vp91/$USER/intro-to-pytorch/data/pima-indians-diabetes.data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee303492-97bf-4274-9a14-04c1c116f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = os.path.expandvars('/scratch/vp91/$USER/intro-to-pytorch/data/pima-indians-diabetes.data.csv')\n",
    "print(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c13d2-bee6-436c-b838-2c8e04a24ec6",
   "metadata": {},
   "source": [
    "### Curate the dataset\n",
    "Load the dataset, split into features (X) and output (y) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a7a99-2723-486d-9a1e-58d2792c84e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(datapath, delimiter=',')\n",
    "X = dataset[:,0:8] \n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a20b9-0c73-4995-b772-0e773cc03c8b",
   "metadata": {},
   "source": [
    "### Convert the data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c45e8f-894e-46d3-84c1-25fbca333f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc70a8b-2c37-4c09-bd7c-717d556cb39c",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "\n",
    "When designing the model, keep the following points in mind:\n",
    "\n",
    "1. The input features in the input layer must match the input features in the dataset (`X_tensor`).\n",
    "2. A high number of layers can increase computation time, while too few layers may result in poor predictions.\n",
    "3. Each layer should be followed by an activation function.\n",
    "\n",
    "In this example, we will use a 3-layer neural network:\n",
    "\n",
    "1. The input layer expects 8 features.\n",
    "2. The first hidden layer has 12 neurons, followed by a ReLU activation function.\n",
    "3. The second hidden layer has 8 neurons, followed by another ReLU activation function.\n",
    "4. The output layer has one neuron, followed by a sigmoid activation function.\n",
    "\n",
    "The sigmoid function outputs values between 0 and 1, which is exactly what we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d8051c-32ee-45c1-b797-58c0e68bbcfb",
   "metadata": {},
   "source": [
    "\n",
    "In PyTorch, neural networks can be defined using different approaches, and two common ones are the Sequential model and the class-based model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a5b5b-d3bc-434f-9b92-a8571d7599f5",
   "metadata": {},
   "source": [
    "#### Sequential model\n",
    "\n",
    "* The Sequential model is a simple, linear stack of layers where each layer has a single input and output. It is useful for straightforward feedforward networks where layers are applied in a sequential order.\n",
    "* It is easier to use for simple architectures where layers are applied in a linear fashion.\n",
    "* Defined Using: *torch.nn.Sequential*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb10442-640a-4ded-a81f-6c2607a86bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = nn.Sequential(\n",
    "    nn.Linear(8, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8647eb7-799b-42e5-b42d-be699b5e5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8225d-4528-4a2f-a23e-9285d4ab5c8e",
   "metadata": {},
   "source": [
    "### Class-Based Model\n",
    "\n",
    "The class-based model allows you to define a network by subclassing torch.nn.Module. This approach provides greater flexibility and control, making it suitable for complex models and custom behaviors.\n",
    "\n",
    "* Offers full control over the network architecture, including complex data flows, multiple inputs/outputs, and custom forward methods.\n",
    "* Custom Forward Pass: You can define complex forward passes and control data flow through the network.\n",
    "* Dynamic Behavior: Allows for dynamic computations, such as conditional layers or operations.\n",
    "* Defined Using: Subclass of torch.nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d5838-320d-4dad-ac59-e2d95ada7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(8, 12)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(12, 8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c55b3-f54b-4a79-ba58-5788237aabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_model = PimaClassifier()\n",
    "print(class_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e889fd5-49a7-4509-a2fc-ef7d5f8c722f",
   "metadata": {},
   "source": [
    "### Define the loss function\n",
    "Binary Cross-Entropy (BCE) Loss: Measures the performance of a classification model whose output is a probability value between 0 and 1. It calculates the difference between the predicted probabilities and the actual binary labels (0 or 1) and penalizes the model more when the predictions are further from the true labels.\n",
    "\n",
    "BCELoss(y', y)=−[ylog(y')+(1−y)log(1−y')]\n",
    "\n",
    "Where, y' is the predicted output and y is the actual otput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593672e5-4e14-473d-80f9-2ed00c127729",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa2eab-7897-439e-9c20-08eb523ec7d6",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Optimizer's main role is to update the model's parameters based on the gradients computed during backpropagation.\n",
    "\n",
    "1. **Parameter Updates**: Optimizers adjust the weights and biases of the neural network to reduce the loss. This involves applying algorithms that modify the parameters to minimize the difference between the predicted outputs and the actual targets.\n",
    "2. **Learning Rate Management**: Most optimizers include mechanisms to adjust the learning rate, either statically or dynamically, to control how large the parameter updates are.\n",
    "\n",
    "In this example we use an optimizer called Adaptive Moment Estimation (Adam). This computes an adaptive learning rates for each parameter by considering both the mean and the variance of the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518bf03c-0594-494e-bdb3-a41421ac53a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(class_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608326d-27a3-4a5b-b485-e9af74b0f2e8",
   "metadata": {},
   "source": [
    "#### Training the Model\n",
    "\n",
    "Training a neural network involves epochs and batches, which define how data is fed to the model:\n",
    "\n",
    "- **Epoch:** A full pass through the entire training dataset.\n",
    "- **Batch:** A subset of samples processed at a time, with gradient descent performed after each batch.\n",
    "\n",
    "In practice, the dataset is divided into batches, and each batch is processed sequentially in a training loop. Completing all batches constitutes one epoch. The process is repeated for multiple epochs to refine the model.\n",
    "\n",
    "Batch size is constrained by system memory (GPU memory), and computational demands scale with batch size. More epochs and batches lead to better model performance but increase training time. The optimal number of epochs and batch size is often determined through experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea1f2ef-0b63-435c-a93f-329aa9ae6228",
   "metadata": {},
   "source": [
    "#### Purpose of optimizer.zero_grad(), loss.backward(), optimizer.step()\n",
    "\n",
    "**optimizer.zero_grad()**: During training, gradients accumulate by default in PyTorch. This means that if you don’t clear them, gradients from multiple backward passes (from different batches) will be added together, which can lead to incorrect updates to the model parameters.\n",
    "By calling optimizer.zero_grad(), you ensure that gradients from previous steps are reset to zero, preventing them from affecting the current update.\n",
    "\n",
    "**loss.backward()**:  Calculates the gradients of the loss with respect to each parameter of the model. This is done using backpropagation, a key algorithm for training neural networks.\n",
    "\n",
    "**optimizer.step()**: Used to update the model's parameters based on the gradients computed during during the backward pass (**loss.backward()**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b09b5-9e9b-4376-ad89-d1ff8b4791eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_epochs = 100\n",
    "batch_size = 10\n",
    " \n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X_tensor), batch_size):\n",
    "        Xbatch = X_tensor[i:i+batch_size]\n",
    "        y_pred = class_model(Xbatch)\n",
    "        ybatch = y_tensor[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b239a9-7e1c-42ba-8b54-58c79d26986b",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "\n",
    "Currently, we are testing the model on the training dataset. Ideally, we should split the data into separate training and testing datasets, or use a distinct dataset for evaluation. For simplicity, we are testing the model on the same data used for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e28fe2-90c5-4cd4-bd37-30ebe9183772",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = class_model(X_tensor)\n",
    " \n",
    "accuracy = (y_pred.round() == y_tensor).float().mean()\n",
    "print(f\"Accuracy {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a44bb-25c6-4f67-8a34-514d7eadbbaf",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. **Increase the number of layers in the neural network.** Observe any changes in accuracy.\n",
    "2. **Change the optimizer from Adam to [Stochastic Gradient Descent (SGD)](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html).** Evaluate how this affects the loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84664807-8163-46dd-b037-1b3f73d8cbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
